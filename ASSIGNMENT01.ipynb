{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOSHY9_ipMYe"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pickle\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"/content/Algerian_forest_fires_cleaned.csv\")\n",
        "\n",
        "\n",
        "df.columns = ['day', 'month', 'year', 'Temperature', 'RH', 'Ws', 'Rain',\n",
        "              'FFMC', 'DMC', 'DC', 'ISI', 'BUI', 'FWI', 'Classes', 'Region']\n",
        "\n",
        "# 3. Data Cleaning and Preprocessing\n",
        "\n",
        "df.drop(columns=['day', 'month', 'year', 'Classes', 'Region'], inplace=True)\n",
        "\n",
        "# Convert data to numeric and drop missing values\n",
        "df = df.apply(pd.to_numeric, errors='coerce')\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# 4. Data Visualization\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "sns.pairplot(df[['Temperature', 'RH', 'Ws', 'Rain', 'FWI']])\n",
        "plt.suptitle(\"Pairplot of Key Features\", y=1.02)\n",
        "plt.show()\n",
        "\n",
        "# 5. Feature and Target Splitting\n",
        "X = df.drop(columns=['FWI'])\n",
        "y = df['FWI']\n",
        "\n",
        "# 6. Train-Test Split and Scaling\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 7. Model Training and Evaluation\n",
        "models = {\n",
        "    'LinearRegression': LinearRegression(),\n",
        "    'Ridge': Ridge(alpha=1.0),\n",
        "    'Lasso': Lasso(alpha=0.1),\n",
        "    'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    preds = model.predict(X_test_scaled)\n",
        "    print(f\"\\n{name} Results:\")\n",
        "    print(\"R2 Score:\", r2_score(y_test, preds))\n",
        "    print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, preds)))\n",
        "\n",
        "# 8. Cross-Validation and Hyperparameter Tuning\n",
        "ridge_params = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
        "ridge_grid = GridSearchCV(Ridge(), ridge_params, cv=5, scoring='r2')\n",
        "ridge_grid.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"\\nBest Ridge Alpha:\", ridge_grid.best_params_)\n",
        "print(\"Best Cross-Validated Score:\", ridge_grid.best_score_)\n",
        "\n",
        "# 9. Save Best Model with Pickle\n",
        "with open(\"best_ridge_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(ridge_grid.best_estimator_, f)\n",
        "\n",
        "# 10. Load Model and Predict on Unseen Data\n",
        "with open(\"best_ridge_model.pkl\", \"rb\") as f:\n",
        "    loaded_model = pickle.load(f)\n",
        "\n",
        "preds_loaded = loaded_model.predict(X_test_scaled)\n",
        "print(\"\\nLoaded Model R2 Score:\", r2_score(y_test, preds_loaded))"
      ]
    }
  ]
}